# Tlama-1  

<div align="center">
  <img src="https://i.postimg.cc/R00W9YMj/Tlama-1.png" alt="EigenCore" width="150" style="margin-right: 50px;">
</div>

Welcome to the **Tlama-1** repository! This project is a collaborative effort between the **EigenCore team** and the **Grupo de Ingeniería Lingüística (GIL)** at the **Universidad Nacional Autónoma de México (UNAM)** to develop an advanced language model that prioritizes Spanish, Indigenous languages of Mexico, and English.  

Tlama-1 is based on the **LLAMA architecture** and is currently in the **development phase**, with pre-training underway. It will be released to the community soon.  

## Introduction  

**Tlama-1** is the first model in a series aimed at revolutionizing natural language processing (NLP) for Spanish and Indigenous languages of Mexico. Our primary goals are:  

- To develop a model capable of understanding and generating text in **Spanish, Indigenous languages of Mexico, and English**.  
- To provide a solid foundation for NLP applications tailored to the needs of Spanish-speaking and Indigenous language communities in Latin America.  

This project not only seeks to advance language model technology but also to promote the inclusion and preservation of Indigenous languages in the digital space.  

## Tlama-1 and Tlama-Core  

The development of **Tlama-1** is closely tied to **tlama-core**, a library being developed by the **EigenCore team** to streamline and optimize the training and fine-tuning of language models.  

### **What is developed in Tlama-1?**  
This repository focuses on the components specific to the **Tlama-1** model, such as:  
- The model architecture.  
- The tokenizer tailored for Spanish, Indigenous languages, and English.  
- Pre-training scripts specific to **Tlama-1**.  

### **What is developed in Tlama-Core?**  
**tlama-core** implements reusable and optimized components that can be used not only for **Tlama-1** but also for other NLP projects. Examples include:  
- Optimized training loop scripts.  
- Custom kernels for common NLP operations.  
- Preprocessing and evaluation tools.  

The goal is for **tlama-core** to serve as a solid foundation, enabling **Tlama-1** and future projects to take off and grow efficiently.  

## Project Status  

Currently, both **Tlama-1** and **tlama-core** are in the **development phase**. We are working on:  

- **Pre-training**: Training the model on a diverse corpus that includes Spanish, Indigenous languages, and English.  
- **Optimization**: Improving the model's performance and efficiency.  
- **Documentation**: Preparing guides and resources to make the project accessible and easy to contribute to.  

Stay tuned for updates! We will share more details about the release soon.  

## Contributing  

Your contributions are valuable to us! If you're interested in contributing, please check out our **[CONTRIBUTING.md](./CONTRIBUTING.md)** file for guidelines. Whether you want to help with model development, improve documentation, or suggest new ideas, your participation is welcome!  

## Team  

Tlama-1 is a project developed by:  

- **EigenCore**: A team dedicated to innovation in artificial intelligence and natural language processing.  
- **Grupo de Ingeniería Lingüística (GIL) - UNAM**: A research group focused on developing linguistic technologies for minority and endangered languages.  

## Contact  

If you have questions, suggestions, or would like to collaborate, feel free to reach out to us via email or social media:

- **EigenCore**: https://eigencore.org/contact
- **GIL - UNAM**: pending

---

### **Additional Notes**  
- **Tlama-1** is the starting point for an ecosystem of tools and models aimed at democratizing access to NLP technologies for Spanish and Indigenous languages.  
- **tlama-core** will be the backbone that enables scaling and optimizing these efforts, facilitating the development of future projects.  
